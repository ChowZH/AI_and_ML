{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43bf216f",
   "metadata": {},
   "source": [
    "# Building Models with PyTorch\n",
    "## `torch.nn.Module` and `torch.nn.Parameter`\n",
    "\n",
    "Here, we'll discuss some of the tools PyTorch makes available for building deep learning networks.\n",
    "\n",
    "Except for `Parameter`, the classes we disciuss in this video are all subclasses of `torch.nn.Module`. This is the PyTorch base class meant to encapsulate behaviours specific to PyTorch Models and their components.\n",
    "\n",
    "One important behaviour of `torch.nn.Module` is registering parameters. If a particular `Module` subclass has learning weights, these weights are expressed as instances of `torch.nn.Parameter`. The `Parameter` class is a subclass of `torch.Tensor`, with the special behaviour that when they are assinged as attributes of a `Module`, they are added to the list of that module's parameters. These parameters may be accessed through the `parameters()` method on the `Module` class.\n",
    "\n",
    "As a simple example, here's a very simple model with two linear layers and an activation function. We'll create an instance of it and task it to report on it's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dfe2cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "\n",
      "Just one layer:\n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "\n",
      "Model params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0490,  0.0928,  0.0199,  ..., -0.0030,  0.0403,  0.0134],\n",
      "        [ 0.0431, -0.0076, -0.0022,  ..., -0.0435, -0.0585,  0.0077],\n",
      "        [ 0.0056, -0.0066,  0.0319,  ...,  0.0839, -0.0310,  0.0756],\n",
      "        ...,\n",
      "        [ 0.0273,  0.0088, -0.0559,  ..., -0.0939, -0.0767,  0.0085],\n",
      "        [-0.0120, -0.0971,  0.0538,  ..., -0.0601, -0.0466, -0.0634],\n",
      "        [-0.0649, -0.0885,  0.0961,  ..., -0.0604,  0.0865,  0.0042]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-3.0698e-02, -8.7677e-03,  6.3464e-02,  8.0533e-02, -8.5202e-02,\n",
      "         4.7646e-02, -4.6430e-02, -3.8487e-02, -2.8487e-02, -6.1240e-03,\n",
      "        -9.1468e-02, -8.6296e-02,  3.8921e-02,  3.4473e-03,  2.8274e-02,\n",
      "        -9.8899e-02,  3.8516e-02, -1.5158e-02,  5.6019e-02,  9.0716e-02,\n",
      "        -1.4802e-02,  9.6895e-02, -9.2586e-02,  5.9660e-04,  1.2354e-02,\n",
      "         1.4611e-02, -4.6349e-02,  7.7075e-02,  9.3660e-03, -7.3201e-02,\n",
      "        -5.9084e-02, -8.5114e-03,  2.9975e-03, -9.8294e-02, -9.2361e-02,\n",
      "        -9.3176e-02,  5.3113e-02,  6.3292e-02,  3.3912e-02, -9.1722e-02,\n",
      "         1.5337e-02, -8.5619e-02,  2.4288e-02,  8.6935e-02, -8.6719e-02,\n",
      "        -7.1005e-03,  5.5006e-02,  4.6164e-02, -9.4113e-02,  4.6159e-02,\n",
      "         1.2232e-02, -5.8453e-02, -3.1259e-02,  4.2340e-02, -7.8507e-03,\n",
      "        -8.9478e-02, -7.8127e-02, -6.6898e-02,  4.2983e-02,  1.9825e-02,\n",
      "         4.8941e-02,  8.6215e-02, -5.2085e-02, -7.7063e-02, -9.2844e-03,\n",
      "        -2.5725e-02, -5.3413e-02, -7.8707e-02,  9.4043e-02,  9.8022e-02,\n",
      "         7.7383e-02, -5.8664e-02, -6.2334e-03,  2.7212e-02,  7.5251e-04,\n",
      "        -1.3881e-02,  9.6705e-02,  5.5115e-02, -5.4031e-03, -1.8178e-02,\n",
      "        -9.3670e-02,  4.0733e-03,  6.5187e-02,  2.9320e-02, -9.3262e-02,\n",
      "        -4.8284e-02, -4.1274e-02, -1.0328e-02,  7.4928e-02, -8.9741e-02,\n",
      "        -3.5087e-02,  3.6587e-02,  7.0446e-02, -6.7782e-02, -2.9707e-02,\n",
      "        -5.9454e-02,  9.5575e-02, -1.8738e-03, -2.6162e-02, -3.9507e-02,\n",
      "        -3.0501e-02,  7.6420e-02, -8.3763e-02,  9.8959e-02,  4.1890e-02,\n",
      "        -3.8590e-02,  3.3355e-03, -9.5929e-02,  5.7160e-02, -8.5363e-02,\n",
      "         2.6075e-02, -3.1445e-02,  8.4175e-02,  2.7577e-02,  5.5714e-02,\n",
      "        -6.8472e-02,  3.2543e-02,  5.4439e-02,  3.2843e-02,  9.8872e-02,\n",
      "        -2.9144e-02,  9.9023e-02, -8.8588e-02,  7.8158e-03,  4.8922e-02,\n",
      "         5.4905e-02,  8.9344e-03,  2.6633e-02,  4.9231e-02,  8.8514e-04,\n",
      "         8.4980e-02,  8.1352e-02, -1.9936e-02, -7.8120e-02, -5.2936e-02,\n",
      "        -2.9894e-02, -8.1256e-02, -1.8106e-02, -9.7214e-02,  6.5640e-02,\n",
      "        -8.7049e-02,  5.5653e-02,  2.1347e-02,  9.2658e-02,  2.9405e-02,\n",
      "         9.6852e-02,  6.6992e-02,  8.1085e-02, -5.0862e-02, -5.9705e-02,\n",
      "        -1.5142e-02,  6.3276e-02, -7.7678e-02, -9.8866e-03,  2.2531e-03,\n",
      "        -7.5631e-02,  8.5699e-02,  4.9848e-02,  2.5359e-02, -7.0912e-02,\n",
      "        -7.0024e-02, -5.2384e-02,  3.0429e-02,  6.6529e-02, -7.0959e-03,\n",
      "         7.2014e-02,  4.9843e-02,  2.9963e-02, -1.0853e-02,  9.8386e-02,\n",
      "        -2.8626e-02,  2.8759e-02,  8.4897e-02, -7.0668e-02, -8.4281e-05,\n",
      "        -6.8869e-02,  3.5165e-02, -9.8541e-02, -3.1473e-02, -2.6827e-02,\n",
      "         1.2543e-02,  9.9403e-03,  4.8183e-02,  5.4175e-02, -1.0010e-02,\n",
      "         7.5167e-02,  4.1077e-02,  5.9402e-02,  8.0757e-02,  1.1037e-02,\n",
      "        -7.2225e-02, -9.6296e-02,  3.6612e-02,  5.0473e-02,  5.0570e-02,\n",
      "         6.1134e-02,  7.0429e-02, -9.1214e-02, -8.7746e-02,  1.8940e-02],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0151, -0.0588,  0.0268,  ..., -0.0183, -0.0295, -0.0243],\n",
      "        [-0.0231, -0.0276,  0.0484,  ...,  0.0611,  0.0580, -0.0265],\n",
      "        [-0.0677,  0.0637, -0.0454,  ..., -0.0419, -0.0303, -0.0692],\n",
      "        ...,\n",
      "        [ 0.0127,  0.0514, -0.0641,  ..., -0.0413,  0.0102,  0.0538],\n",
      "        [ 0.0639,  0.0591, -0.0197,  ...,  0.0410,  0.0268,  0.0030],\n",
      "        [ 0.0508, -0.0370, -0.0584,  ...,  0.0360, -0.0072, -0.0246]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0693, -0.0279,  0.0523, -0.0337,  0.0468,  0.0475, -0.0281, -0.0614,\n",
      "         0.0165,  0.0325], requires_grad=True)\n",
      "\n",
      "\n",
      "Layer params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0151, -0.0588,  0.0268,  ..., -0.0183, -0.0295, -0.0243],\n",
      "        [-0.0231, -0.0276,  0.0484,  ...,  0.0611,  0.0580, -0.0265],\n",
      "        [-0.0677,  0.0637, -0.0454,  ..., -0.0419, -0.0303, -0.0692],\n",
      "        ...,\n",
      "        [ 0.0127,  0.0514, -0.0641,  ..., -0.0413,  0.0102,  0.0538],\n",
      "        [ 0.0639,  0.0591, -0.0197,  ...,  0.0410,  0.0268,  0.0030],\n",
      "        [ 0.0508, -0.0370, -0.0584,  ...,  0.0360, -0.0072, -0.0246]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0693, -0.0279,  0.0523, -0.0337,  0.0468,  0.0475, -0.0281, -0.0614,\n",
      "         0.0165,  0.0325], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear (200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "tinymodel = TinyModel()\n",
    "\n",
    "print ('The model:')\n",
    "print (tinymodel)\n",
    "\n",
    "print ('\\n\\nJust one layer:')\n",
    "print (tinymodel.linear2)\n",
    "\n",
    "print ('\\n\\nModel params:')\n",
    "for param in tinymodel.parameters():\n",
    "    print (param)\n",
    "    \n",
    "print ('\\n\\nLayer params:')\n",
    "for param in tinymodel.linear2.parameters():\n",
    "    print (param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684389f",
   "metadata": {},
   "source": [
    "This shows the fundamental structure of a PyTorch model: there is an `__init__()` method that defines the layers and other components of a model, and a `forward()` method where the computation gets done. Note that we can print the model, or any of its submodules to learn about its structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab7a6eb",
   "metadata": {},
   "source": [
    "## Common Layer Types\n",
    "\n",
    "### Linear Layers\n",
    "The most basic type of a neural network is a _linear_ or _fully conencted_ layer. This is a layer where every input influences every output of the layer to a degree specified by the layer's weights. If a model has _m_ inputs, and _n_ outputs, the weights will be an _m x n_ matrix. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "166bcb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[0.4231, 0.8234, 0.6379]])\n",
      "\n",
      "\n",
      "Weight and Bias parameters:\n",
      "Parameter containing:\n",
      "tensor([[ 0.2999,  0.5121, -0.2398],\n",
      "        [ 0.3709, -0.2685, -0.0796]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3193, 0.0170], requires_grad=True)\n",
      "\n",
      "\n",
      "Output:\n",
      "tensor([[ 0.7148, -0.0980]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lin = torch.nn.Linear(3, 2)\n",
    "x = torch.rand(1, 3)\n",
    "print ('Input:')\n",
    "print (x)\n",
    "\n",
    "print ('\\n\\nWeight and Bias parameters:')\n",
    "for param in lin.parameters():\n",
    "    print (param)\n",
    "    \n",
    "y = lin(x)\n",
    "print ('\\n\\nOutput:')\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a1593",
   "metadata": {},
   "source": [
    "If we do the matrix multiplication of `x` by the linear layer's weights, and add the biases, we get the output vector `y`.\n",
    "\n",
    "One other important feature of note: When we checked the weights of our layer with `lin.weight`, it reported itself as a `Parameter` (which is a subclass of `Tensor`), and let us know that it's tracking gradients with autograd. This is a default behavious for `Parameter` that deffiers from `Tensor`.\n",
    "\n",
    "Linear layers are used widely in deep learning models. One of the most common places we'll see them is in classifier models, which will usually have one or more linear layers at the end where the last layer will have _n_ outputs, where _n_ is the number of classes the classifier addresses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bc4b76",
   "metadata": {},
   "source": [
    "### Convolutional Layers\n",
    "_Convolutional_ layers are built to handle data with high degree of spatial correlation. They are very commonly used in computer vision, where they detect close groupings of features which they compose into higher-level features. They pop up in other contexts too - for example, in NLP applications, where a word's immediate context (that is, the other words nearby in the sequence) can affect the meaning of the sentence.\n",
    "\n",
    "We saw convolutional layers in action in LeNet5 before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b27100a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.con1 = torch.nn.Conv2d(1, 6, 5)\n",
    "        self.con2 = torch.nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = torch.nn.Linear(16*6*6, 120) # 6*6 frm image dimension\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ea60f",
   "metadata": {},
   "source": [
    "Let's breakd down the code cell above. Starting with `conv1`:\n",
    "- LeNet5 is meant to take in a 1x32x32 black & white image. **The first argument to a convolutional layer's constructor is the number of input channels.** Here, it is 1. If we were building this model to look at 3-colour channels, it would be 3.\n",
    "- A convolutional layer is liek a window that scans over the image, looking for a pattern it recognises. These patterns are called _features_, and one of the parameters of a convolutional layer is teh number of features we would like to learn. **This is the second argument to the constructor; the number of output features.** Here, we're asking our layer to learn 6 features.\n",
    "- Just above, we likened the convolutional layer to a window - but how big is the window? **The third argument is the window or kernel size.** Here, the \"5\" means we've chosen a 5x5 kernel. (If we want a kernel with height different from width, we can specify a tuple for this argument - e.g., `(3, 5)` to get a 3x5 convolution kernel.)\n",
    "\n",
    "The output of a convolutional layer is teh _activation map_ - a spatial representation of the presence of features in the input tensor. `conv1` will give us an output tensor of 6x28x28; 6 is the number of features, and 28 is the height and width of our map. (The 28 comes from the fact that when scanning a 5-pixel window over a 32-pixel row, there are only 28 valid positions.)\n",
    "\n",
    "We then pass the output of the convolution through a ReLU activation function (more on activation functions later), then through a max pooling layer. The max pooling layer takes features near each other int eh activation map, and groups them together. It does this by reducing the tensor, merging every 2x2 group of cells in the output into a single cell, and assigning that cell the maximum of the 4 cells that went into it. This gives us a lower-resolution version of the activation map, with dimensions 6x14x14.\n",
    "\n",
    "Our next convolutional layer, `conv2`, expects 6 input channels (corresponding to the 6 features sought by the first layer), has 16 output channels, and a 3x3 kernel. It puts out a 16x12x12 activation map, which is again reduced by a max pooling layer to 16x6x6. Prior to passing this output to the linear layers, it is reshaped to a 16 \\* 6 \\* 6 = 576-element vector for consumption by the next layer.\n",
    "There are convolutional layers addressing 1D, 2D, and 3D tensors. There are also many more optional aguments for a conv layer constructor, including stride length (e.g., only scanning every second or every third position) in the input, padding (so we can scan out to the edges of the input), and more. See the [documentation](https://pytorch.org/docs/stable/nn.html#convolution-layers) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4029b661",
   "metadata": {},
   "source": [
    "### Recurrent Layers\n",
    "\n",
    "_Recurrent Neural Networks_ (or RNNs) are used for sequential data - anything from time-series measurements from a scientific instrument to natural language sentences to DNA neucleotides. An RNN does this by maintaining a _hidden state_ that acts as a sort of memory for what it has seen in the sequence so far.\n",
    "\n",
    "The internal structure of an RNN layer - or its variants, the LSTM (long short-term memory) and GRU (gated recurrent unit) - is moderately complex and beyond the scope of this document, but we will see what one looks like in action with an LSTM-based part-of-speech tagger (a type of classifier that tells if a word is a noun, verb, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d18d08ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # The linear layer that maps gfrom hidden state space to tag space\n",
    "        self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scrores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a8176e",
   "metadata": {},
   "source": [
    "The contructor has 4 arguments:\n",
    "- `vocab_size` is the number of words in the input vocabulary. Each word is a one-hot vector (or unit vector) in a `vocab_size`-dimensional space.\n",
    "- `tagset_size` is the number of tags in teh output set.\n",
    "- `embedding_dim` is the size of the _embedding_ space for the vocabulary. An embedding maps a vocabulary onto a low-dimensional space, where words with similar meanings are closer together in the space.\n",
    "- `hidden_dim` is the size of the LSTM's memory.\n",
    "\n",
    "The input will be a sentence with the words represented as indices of one-hot vectors. The mebedding layer will then map these down to an `embedding_dim`-dimensional space. The LSTM takes this sequence of embeddings and iterates over it. Fielding an output vector of length `hidden_dim`. The final linear layer acts as a classifier; applying `log_softmax()` to the output of the final layer convers the output into a normalized set of estimated probabilities that a given word maps to a given tag.\n",
    "\n",
    "To see the network in action, check out the [Sequence Models and LSTM Networks](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) tutorial on pytorch.org."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfedc2e",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "\n",
    "_Transformers_ are multi-purpose networks that have taken over the state-of-the-artin NLP with models like BERT. A discussion of transformer architecture is beyond the scope of this document, but PyTorch has a `Transformer` class that allows us to define the overall parameters of a transformer model - the number of attention heads, the number of encoder & decoder layers, dropout and activation functions, etc. (We can even build the BERT model from this single class, with the right parameters!) The `Torch.nn.Transformer` class also has classes to encapsulate the individual components (`TransformerEncoderLayer`, `TransformerDecoderLayer`). For details, check the [documentation](https://pytorch.org/docs/stable/nn.html#transformer-layers) on transformer classes, and the relevant [tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) on pytorch.org."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a11203b",
   "metadata": {},
   "source": [
    "## Other Layers and Functions\n",
    "### Data Manipulation Layers\n",
    "There are other layer types that perform important functions in models, but don't participate in the learning process themselves.\n",
    "\n",
    "**Max pooling** (and its twin, min pooling) reduce a tensor by combining cells, and assigning the maximm value of the input cells to the output cell (we saw this). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1479fbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0549, 0.8156, 0.8104, 0.6509, 0.6754, 0.6188],\n",
      "         [0.6831, 0.8625, 0.1405, 0.2928, 0.5995, 0.3090],\n",
      "         [0.5759, 0.0909, 0.2288, 0.7346, 0.1516, 0.1940],\n",
      "         [0.3257, 0.5399, 0.1833, 0.3616, 0.2169, 0.3505],\n",
      "         [0.2414, 0.7608, 0.0111, 0.7858, 0.0402, 0.8128],\n",
      "         [0.5773, 0.4504, 0.2304, 0.6537, 0.2951, 0.6097]]])\n",
      "tensor([[[0.8625, 0.7346],\n",
      "         [0.7608, 0.8128]]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 6, 6)\n",
    "print (my_tensor)\n",
    "\n",
    "maxpool_layer = torch.nn.MaxPool2d(3)\n",
    "print (maxpool_layer(my_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f99723",
   "metadata": {},
   "source": [
    "Looking closely at the values above, we see that each of the values in the maxpooled output is the maximum value of each quadrant of the 6x6 input.\n",
    "\n",
    "**Normalization layes** re-center and normalize the output of one layer before feeding it to another. Centering and scaling the intermediate tensors has a number of beneficial effects, such as letting us use higher learning rates without exploding/vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ab78fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[15.0707, 24.1291, 16.2062, 24.8783],\n",
      "         [ 6.7288, 22.1057, 11.9832,  7.8761],\n",
      "         [ 9.5640,  9.3455, 17.0561, 23.4175],\n",
      "         [ 5.2270,  8.3228, 12.3505, 24.0879]]])\n",
      "tensor(14.8968)\n",
      "tensor([[[-1.1215,  0.9101, -0.8668,  1.0782],\n",
      "         [-0.8987,  1.6395, -0.0314, -0.7094],\n",
      "         [-0.9041, -0.9415,  0.3784,  1.4673],\n",
      "         [-1.0164, -0.5836, -0.0205,  1.6205]]],\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "tensor(-3.7253e-08, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 4, 4) * 20 + 5\n",
    "print (my_tensor)\n",
    "\n",
    "print (my_tensor.mean())\n",
    "\n",
    "norm_layer = torch.nn.BatchNorm1d(4)\n",
    "normed_tensor = norm_layer(my_tensor)\n",
    "print (normed_tensor)\n",
    "\n",
    "print (normed_tensor.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903d31ca",
   "metadata": {},
   "source": [
    "Running the cell above, we've added a large scaling factor and offset to an input tensor; we see the input tensor's `mean()` is somewhere around 15. After running it through the normalization layer, we see that the values are smaller, and grouped around zero - in fact, the mean should be very small (>1e-8).\n",
    "\n",
    "This is beneficial because many activation functions (discussed below) have their strongest gradients near 0, but sometimes suffer from vanishing or exploding gradients for inputs that drive them far away from zero. Keeping the data centered around the area of steepest gradient will tend to mean faster, better learning and higher feasible learning rates.\n",
    "\n",
    "**Dropout layers** are a tool for encouraging _sparse representations_ in our model - that is, pushing it to do inference with less data.\n",
    "\n",
    "Dropout layers work by randomly setting parts of the input tensor off _during training_ - dropout layers are always turned off for inference. This forces the model to learn against this masked or reduced dataset. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8367168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 1.6511, 0.3868, 0.0000],\n",
      "         [0.5219, 1.1435, 0.0000, 1.3716],\n",
      "         [0.0000, 0.0000, 0.9656, 0.5082],\n",
      "         [0.2845, 0.0773, 0.2342, 0.0000]]])\n",
      "tensor([[[1.2364, 0.0000, 0.0000, 0.1478],\n",
      "         [0.5219, 1.1435, 0.0000, 1.3716],\n",
      "         [0.0000, 1.2196, 0.9656, 0.0000],\n",
      "         [0.2845, 0.0773, 0.2342, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 4, 4)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.4)\n",
    "print (dropout(my_tensor))\n",
    "print (dropout(my_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bfc958",
   "metadata": {},
   "source": [
    "Above, we see the effect of dropout on a sample tensor. We can use the optional `p` argument to set the probability of an individual weight dropping out; is we don't the default is 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5baaffa",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "Activation functions make deep learning possible. A neural network is really a program - with many parameters - that _simulates mathematical function_. If all we did was multiply tensors by layer weights repeatedly, we could only simulate _linear functions_; further, there would be no point to having many layers, sa the whole network would reduce to a single matrix multiplication. Inserting _non-linearity_ in the form of activation functions between layers is what allows a deep learning model to simulate any function, rather than just linear ones.\n",
    "\n",
    "`torch.nn.Module` has objects encapsulating all of the major activation functions including ReLU and its many variants, Tanh, Hardtanh, sigmoid and more. It also includes other functions, such as Softmax, that are most useful at eh output stage of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebf9d2a",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "Loss functions tell us how far a model's prediction is from the correct answer. PyTorch contains a variety of loss functions, including common MSE (mean sqiared error = L2 norm), Cross Entropy Loss and Negative Likelihood Loss (useful for classifiers), and others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
