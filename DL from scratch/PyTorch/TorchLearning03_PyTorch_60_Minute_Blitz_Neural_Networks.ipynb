{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb61580b",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "Neural networks can be constructed using the `torch.nn` package.\n",
    "\n",
    "`nn` depends on `autograd` to define models and differentiate them. An `nn.Module` contains layers, and a method `forward(input)` that returns the `output`.\n",
    "\n",
    "For example, below is a look at a letter classifier:\n",
    "![](https://pytorch.org/tutorials/_images/mnist.png)\n",
    "\n",
    "It is a simple feed-forward network, taking in an input, feeds it through several layers one after the other, and finally produce an output.\n",
    "\n",
    "A typical training procedure for a neural network is as follows:\n",
    "- Define the neural network that has some learnable parameters (or weights)\n",
    "- Iterate overa dataset of inputs\n",
    "- Process input through the network\n",
    "- Compute the loss (how far is the output from being correct)\n",
    "- Propagate gradients back intop the network's parameters\n",
    "- Update the weights of the network, typically using a simple update rulew: `weight = weight - learning_rate * gradient`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67afd6e7",
   "metadata": {},
   "source": [
    "## Define the Network:\n",
    "Let's define this netrwork:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7c54021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 1 input channel, 6 output features, 5x5 kernels\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x,1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "print (net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a5fd0e",
   "metadata": {},
   "source": [
    "With this, a `forward` function is defined, and `backward` can be called to compute the gradients through `autograd`. We can use any of the Tensor operations in the `forward` function.\n",
    "\n",
    "The learnable parameters of a model are returned by `net.parameters()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad6d0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print (len(params))\n",
    "print (params[0].size()) # weights of conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae417fa",
   "metadata": {},
   "source": [
    "Let's try a random 32x32 input. Note: expected input size of this net(LeNet) is 32x32. To use this net on the MNIST dataset, please resize the images from the dataset to 32x32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eea577f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0357,  0.0771, -0.0261, -0.0875,  0.0452,  0.2251, -0.0068, -0.0493,\n",
      "         -0.0207, -0.0633]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print (out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14acd82",
   "metadata": {},
   "source": [
    "Zero the gradient buffers of all parameters and backpros with random gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff7df9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9ba02f",
   "metadata": {},
   "source": [
    "Note: `torch.nn` only supports mini-batches. The entire `torch.nn` package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
    "\n",
    "For example, `nn.Conv2d` will take in a 4D Tensor of `nSamples x nChannels x Height x Width`.\n",
    "\n",
    "If we have a single sample, just use `input.unsqueeze(0)` to add an empty dimension of axis 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b1cc11",
   "metadata": {},
   "source": [
    "### Recap:\n",
    "- `torch.Tensor` - A _multi-dimensional array_ with support for autograd operations like `backward()`. Also _holds gradients_ w.r.t. the tensor.\n",
    "- `nn.Module` - Neural Network module. _Convevient way of encapsulating parameters_, with helpers for moving them to GPU, exporting, loading, etc.\n",
    "- `nn.Parameters` - A Tensor _automatically registered as a parameter when assigned as an attribute_ to a `Module`.\n",
    "- `autograd.Function` - Implements _forward_ and _backward_ definitions of an _autograd operation_. Every `Tensor` operation creates at least a single `Function` node that connects to functions that created a `Tensor` and _encodes its history_.\n",
    "- We also defined a neural network class with `nn.Module`\n",
    "- Demonstrated processing inputs and the `.backward()` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e22fcd1",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "A loss function is a measure of how far away the output is from the target. There are several [loss functions](https://pytorch.org/docs/nn.html#loss-functions) under the `nn` package. A commonly used loss function is `nn.MSELoss` which computes the mean-squared error between the output and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4883139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0993, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10) # dummy target\n",
    "target = target.view(1, -1) # make it same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print (loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1155329a",
   "metadata": {},
   "source": [
    "If we follow `loss` in the backward direction, using its `.grad_fn` attribute, we see a graph of comutations that look like:\n",
    "\n",
    "`input  -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d \n",
    "        -> flatten -> linear -> relu -> linear -> relu -> linear\n",
    "        -> MSELoss\n",
    "        -> loss\n",
    "`\n",
    "\n",
    "So when we call `loss.backward()` the whole graph is differentiated w.r.t. the neural network parameters, and all Tensors in the graph that have `requires_grad=True` will have their `.grad` Tensor accumulated with the gradient.\n",
    "\n",
    "For illustration, let's follow a few steps backward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64a3d4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward0 object at 0x00000191A4AE5580>\n",
      "<AddmmBackward0 object at 0x00000191A4AE5A30>\n",
      "<AccumulateGrad object at 0x00000191A4AE5580>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8922751",
   "metadata": {},
   "source": [
    "## Backprop\n",
    "To backpropagate the error, all we have to do is call `loss.backward()`. We need to clear the existing gradients, else new gradients will be accumulated onto existing gradients.\n",
    "\n",
    "Now we call `loss.backward()`, and have a look at conv1's bias gradients before and fater the backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d241f7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad before backward\n",
      "tensor([ 0.0068, -0.0036,  0.0027, -0.0134,  0.0136, -0.0031])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "\n",
    "print ('conv1.bias.grad before backward')\n",
    "print (net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "print ('conv1.bias.grad before backward')\n",
    "print (net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f22a015",
   "metadata": {},
   "source": [
    "## Update the Weights\n",
    "With the gradients propagated backwrds, we can now use them to update the weights of our network. A common optimization method is stochastic gradient descent (SGD):\n",
    "\n",
    "`weight = weight - learning_rate * gradient`\n",
    "\n",
    "We can implement this using Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "466227d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd513bb",
   "metadata": {},
   "source": [
    "However, PyTorch provides a more flexible library that allows us to change optimization methods from SGD to Adam, RMSPros, etc. with the `torch.optim` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "264e5caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr = 1e-2)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "output = net(input)\n",
    "loss = criterion (output, target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e16dea",
   "metadata": {},
   "source": [
    "Note: Observe how the buffers had to be manually set to zero using `optimizer.zero_grad()`. This is bacause gradients are accumulated by default as explained previously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
