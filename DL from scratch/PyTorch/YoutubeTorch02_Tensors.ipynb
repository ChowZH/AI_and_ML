{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96a45e3c",
   "metadata": {},
   "source": [
    "# PyTorch Tensors:\n",
    "Tensors are the central data abstration in PyTorch. This interactive notebook provides an in-depth introduction to the `torch.Tensor` class.\n",
    "\n",
    "First, we import the PyTorch modules. We also import Python's math module for some comparative examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c5203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b116a3",
   "metadata": {},
   "source": [
    "## Creating Tensors:\n",
    "The simplest way to create tensors is with the `torch.empty()` call. This creates an empty tensor with the specified dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f4f5d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [2.8026e-45, 0.0000e+00, 1.1210e-44, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(3, 4)\n",
    "\n",
    "print (type(x))\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef376de",
   "metadata": {},
   "source": [
    "Let's unpack the code:\n",
    "- We created a tensor using one of the factory methods attached to the `torch` module.\n",
    "- The tensor itself is 2 dimensional, 3 rows and 4 columns.\n",
    "- The type of object returned in `torch.Tensor`, which is an alias for `torch.FloatTensor`; by default, Pytorch tensors are populated by 32-bit floating point numbers.\n",
    "- While the values may sometimes differ, it is because the `torch.empty()` call only allocates memory for the tensor and not initialize them with any values; values or non-values are simply whatever was in memory at time of allocation.\n",
    "\n",
    "A brief note about tensors, their number of dimensions, and terminology:\n",
    "- A 1-dimensional tensor is sometimes called a _vector_.\n",
    "- A 2-dimensional tensor is sometimes called a _matrix_.\n",
    "- Anything with more than 2 dimensions are generally called _tensors_.\n",
    "\n",
    "More often than not, tensors are initialized with some values. We can itialize them with different values with different calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "775a1dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a zeros matrix:\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "\n",
      "\n",
      "This is a ones matrix:\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "\n",
      "\n",
      "This is a random matrix:\n",
      "tensor([[0.3126, 0.3791, 0.3087, 0.0736],\n",
      "        [0.4216, 0.0691, 0.2332, 0.4047],\n",
      "        [0.2162, 0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "\n",
      "This is an identity matrix:\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.zeros(3, 4)\n",
    "\n",
    "ones = torch.ones(3, 4)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "rand = torch.rand(3, 4)\n",
    "\n",
    "## Zeros matrix\n",
    "print (f\"This is a zeros matrix:\")\n",
    "print (zeros)\n",
    "print (\"\\n\")\n",
    "\n",
    "print (f\"This is a ones matrix:\")\n",
    "print (ones)\n",
    "print (\"\\n\")\n",
    "\n",
    "print (f\"This is a random matrix:\")\n",
    "print (rand)\n",
    "print (\"\\n\")\n",
    "\n",
    "identity = torch.eye(4)\n",
    "\n",
    "print (f\"This is an identity matrix:\")\n",
    "print (identity)\n",
    "print (\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbc3d93",
   "metadata": {},
   "source": [
    "The factory methods do as we expect, `torch.zeros` and `torch.ones` initialize matrices with zeros and ones with the specified shape respectively. `torch.rand` populates the matrix with random numbers bewteen 0 and 1. `torch.eye` takes in one integer, k, as a parameter and returns a kxk identity matrix.\n",
    "\n",
    "For `torch.rand` we can also specify seeds to facilitate reproductibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0da22b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a random 4x4 matrix with seed = 1234:\n",
      "tensor([[0.0290, 0.4019, 0.2598, 0.3666],\n",
      "        [0.0583, 0.7006, 0.0518, 0.4681],\n",
      "        [0.6738, 0.3315, 0.7837, 0.5631],\n",
      "        [0.7749, 0.8208, 0.2793, 0.6817]]) \n",
      "\n",
      "This is a random 4x4 matrix with no given seed:\n",
      "tensor([[0.2837, 0.6567, 0.2388, 0.7313],\n",
      "        [0.6012, 0.3043, 0.2548, 0.6294],\n",
      "        [0.9665, 0.7399, 0.4517, 0.4757],\n",
      "        [0.7842, 0.1525, 0.6662, 0.3343]]) \n",
      "\n",
      "This is another matrix but with the same seed = 1234:\n",
      "tensor([[0.0290, 0.4019, 0.2598, 0.3666],\n",
      "        [0.0583, 0.7006, 0.0518, 0.4681],\n",
      "        [0.6738, 0.3315, 0.7837, 0.5631],\n",
      "        [0.7749, 0.8208, 0.2793, 0.6817]]) \n",
      "\n",
      "This is another matrix without the seed:\n",
      "tensor([[0.2837, 0.6567, 0.2388, 0.7313],\n",
      "        [0.6012, 0.3043, 0.2548, 0.6294],\n",
      "        [0.9665, 0.7399, 0.4517, 0.4757],\n",
      "        [0.7842, 0.1525, 0.6662, 0.3343]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "r1 = torch.rand(4, 4)\n",
    "\n",
    "r2 = torch.rand(4, 4)\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "r3 = torch.rand(4, 4)\n",
    "\n",
    "r4 = torch.rand(4, 4)\n",
    "\n",
    "print (\"This is a random 4x4 matrix with seed = 1234:\")\n",
    "print (r1,'\\n')\n",
    "\n",
    "print (\"This is a random 4x4 matrix with no given seed:\")\n",
    "print (r2,'\\n')\n",
    "\n",
    "print (\"This is another matrix but with the same seed = 1234:\")\n",
    "print (r3,'\\n')\n",
    "\n",
    "print (\"This is another matrix without the seed:\")\n",
    "print (r4,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb970877",
   "metadata": {},
   "source": [
    "As we can see, values of r1 and r3, and r2 and r4 are the same. Manually resetting the seed before r3 to that it was in r1 makes it so that following computations return identical results. \n",
    "\n",
    "More information on PyTorch reproducibility is available [here](https://pytorch.org/docs/stable/notes/randomness.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cefc02a",
   "metadata": {},
   "source": [
    "## Tensor Shape:\n",
    "Often when performing operations on tensors, we need it to be of compatible _shape_ - that is having the same shape for additions and subtractions, or the right dimensions for matrix multiplications or dot products. To create matrices of the same shape, we have the `torch.*_like()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f61779ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0.7893, 0.3216, 0.5247],\n",
      "         [0.6688, 0.8436, 0.4265]],\n",
      "\n",
      "        [[0.9561, 0.0770, 0.4108],\n",
      "         [0.0014, 0.5414, 0.6419]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(2, 2, 3)\n",
    "print (x.shape)\n",
    "print (x)\n",
    "\n",
    "empty_like_x = torch.empty_like(x)\n",
    "print (empty_like_x.shape)\n",
    "print (empty_like_x)\n",
    "\n",
    "zeros_like_x = torch.zeros_like(x)\n",
    "print (zeros_like_x.shape)\n",
    "print (zeros_like_x)\n",
    "\n",
    "ones_like_x = torch.ones_like(x)\n",
    "print (ones_like_x.shape)\n",
    "print (ones_like_x)\n",
    "\n",
    "rand_like_x = torch.rand_like(x)\n",
    "print (rand_like_x.shape)\n",
    "print (rand_like_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1306b07f",
   "metadata": {},
   "source": [
    "In the above cell, we first defined the shape of an empty tensor `x`. We then used `torch.empty_like(x)`, `torch.zeros_like(x)`,  `torch.ones_like(x)` and `torch.rand_like(x)` to make tensors with the same dimensions.\n",
    "\n",
    "Another way to create tensors is to specify the data directly from a PyTorch collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d23a69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1416, 2.7183],\n",
      "        [1.6180, 0.0073]])\n",
      "tensor([ 2,  3,  5,  7, 11, 13, 17, 19])\n",
      "tensor([[2, 4, 6],\n",
      "        [3, 6, 9]])\n"
     ]
    }
   ],
   "source": [
    "some_constants = torch.tensor([[3.1415926, 2.71828], [1.61803, 0.0072897]])\n",
    "print (some_constants)\n",
    "\n",
    "some_integers = torch.tensor((2, 3, 5, 7, 11, 13, 17, 19))\n",
    "print (some_integers)\n",
    "\n",
    "more_integers = torch.tensor(((2, 4, 6), [3, 6, 9]))\n",
    "print (more_integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46369f7",
   "metadata": {},
   "source": [
    "Using `torch.tensor()` is the most straightforward way to create a tensor if we already have a Python tuple or list.\n",
    "\n",
    "Note: `torch.tensor` creates a copy of the data. It does not replace the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3239e133",
   "metadata": {},
   "source": [
    "## Tensor Data Types:\n",
    "Setting the tensor datatype is possible in a number of ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5639fc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specify datatype as 16 bit integer:\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "torch.int16\n",
      "\n",
      "Specify datatype as 64 bit floating point number:\n",
      "tensor([[7.6155, 7.0313, 1.4186],\n",
      "        [0.4486, 9.4176, 6.3867]], dtype=torch.float64)\n",
      "torch.float64\n",
      "\n",
      "Specify datatype as conversion to 32 bit integer:\n",
      "tensor([[7, 7, 1],\n",
      "        [0, 9, 6]], dtype=torch.int32)\n",
      "torch.int32\n",
      "\n",
      "Default datatype (32 bit floating point number):\n",
      "tensor([[0.1100, 0.2541, 0.4333],\n",
      "        [0.4451, 0.4966, 0.7865]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 3), dtype=torch.int16)\n",
    "\n",
    "b = torch.rand((2, 3), dtype=torch.float64) * 20\n",
    "\n",
    "c = b.to(torch.int32)\n",
    "\n",
    "d = torch.rand(2, 3)\n",
    "\n",
    "print (\"Specify datatype as 16 bit integer:\")\n",
    "print (a)\n",
    "print (a.dtype)\n",
    "\n",
    "print (\"\\nSpecify datatype as 64 bit floating point number:\")\n",
    "print (b)\n",
    "print (b.dtype)\n",
    "\n",
    "print (\"\\nSpecify datatype as conversion to 32 bit integer:\")\n",
    "print (c)\n",
    "print (c.dtype)\n",
    "\n",
    "print (\"\\nDefault datatype (32 bit floating point number):\")\n",
    "print (d)\n",
    "print (d.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbbd18e",
   "metadata": {},
   "source": [
    "The simplest way to set datatypes is with an optional argument at creation time. In `a` we define `dtype=torch.int16`, resulting in the values being `1` instead of `1.`.\n",
    "\n",
    "While we group the dimensions of the tensor as a tuple, this is not strictly necessary. PyTorch takes the series of unlabeled integers as arguments for tensor shape, but adding brackets as formatting facilitates reading.\n",
    "\n",
    "Another way to set the datatype is with the `.to()` method. In the cell above, we create a random 64-bit float tensor `b` and convert the values to 32 bit integers with the `.to()` method.\n",
    "\n",
    "If unspecified, PyTorch sets the datatypes to 32-bit float as in `d`.\n",
    "\n",
    "Available datatypes include:\n",
    "- `torch.bool`\n",
    "- `torch.int8`\n",
    "- `torch.int16`\n",
    "- `torch.int32`\n",
    "- `torch.int64`\n",
    "- `torch.half`\n",
    "- `torch.float`\n",
    "- `torch.double`\n",
    "- `torch.bfloat`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f470908",
   "metadata": {},
   "source": [
    "## Math and Logic with PyTorch Tensors\n",
    "After creating tensors, we need to manipulate them. For demonstration, we create some tensors using operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f1799d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n",
      "tensor([[1.4142, 1.4142],\n",
      "        [1.4142, 1.4142]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.zeros(2,2) + 1\n",
    "twos = torch.ones(2,2) * 2\n",
    "threes = (torch.ones(2,2) * 7 - 1)/2\n",
    "fours = twos ** 2\n",
    "sqrt2s = twos ** 0.5\n",
    "\n",
    "print (ones)\n",
    "print (twos)\n",
    "print (threes)\n",
    "print (fours)\n",
    "print (sqrt2s)\n",
    "\n",
    "alt_threes = ones + twos\n",
    "alt_fours = threes + (twos/twos)\n",
    "\n",
    "print (alt_threes)\n",
    "print (alt_fours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c62014",
   "metadata": {},
   "source": [
    "As demonstrated, arithmatic operations between tensors and scalars such as addition, multiplication, division, and exponentiation are distributed over all elements of the tensor. Similar operations bewteen two tensors behave as we intuitively expect.\n",
    "\n",
    "It is important to note the tensors are of the same shape. Performing arithmatic operations on tensors of dissimilar shapes produces errors.\n",
    "\n",
    "The exception to the same-shape rule is with _tensor broadcasting_. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b59f9454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6604, 0.1303, 0.3498, 0.3824],\n",
      "        [0.8043, 0.3186, 0.2908, 0.4196]])\n",
      "tensor([[1.3208, 0.2606, 0.6996, 0.7648],\n",
      "        [1.6086, 0.6372, 0.5816, 0.8392]])\n"
     ]
    }
   ],
   "source": [
    "rand = torch.rand(2, 4)\n",
    "\n",
    "doubled  = rand * (torch.ones(1, 4)*2)\n",
    "\n",
    "print (rand)\n",
    "print (doubled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273eda3a",
   "metadata": {},
   "source": [
    "The trick to broadcasting is:\n",
    "- Each tensor must have at least 1 dimension.\n",
    "- Comparing the dimension sizes of the two tensors, _going from last to first_\n",
    "    - Each dimension must be equal, _or_\n",
    "    - One of the dimensions must be size 1, _or_\n",
    "    - The dimension does not exist in one of the tnsors\n",
    "\n",
    "Tensors of identical shape are trivially \"broadcastable\", as the earlier examples.\n",
    "\n",
    "Here are some other examples of broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7104daa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "tensor([[[0.1880, 0.1880],\n",
      "         [0.5174, 0.5174],\n",
      "         [0.7849, 0.7849]],\n",
      "\n",
      "        [[0.1880, 0.1880],\n",
      "         [0.5174, 0.5174],\n",
      "         [0.7849, 0.7849]],\n",
      "\n",
      "        [[0.1880, 0.1880],\n",
      "         [0.5174, 0.5174],\n",
      "         [0.7849, 0.7849]],\n",
      "\n",
      "        [[0.1880, 0.1880],\n",
      "         [0.5174, 0.5174],\n",
      "         [0.7849, 0.7849]]])\n",
      "torch.Size([4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4, 3, 2)\n",
    "\n",
    "b = a * torch.rand(3, 2) # second and third dimensions are identical to a, and dim 1 absent\n",
    "\n",
    "c = a * torch.rand(3, 1) # third dimension is 1, second dimension identical to a, and dim 1 absent\n",
    "\n",
    "d = a * torch.rand(1, 2) # third dimension is identical to a, second dimension is 1, and dim 1 absent\n",
    "\n",
    "print (a.shape)\n",
    "print (b.shape)\n",
    "print (c)\n",
    "print (d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f678f3",
   "metadata": {},
   "source": [
    "If we look at the values of each tensor above:\n",
    "- The multiplication operation that created `b` was broadcasted over every \"layer\" of `a`. (dim 1)\n",
    "- For `c`, the operation was broadcasted over every layer and row of `a` - every 3-element column is identical. (dim 1, 3)\n",
    "- For `d`, the operation was switched around, making every row identical. (dim 1, 2)\n",
    "\n",
    "For more information on broadcasting, a more detailed documention is available [here](https://pytorch.org/docs/stable/notes/broadcasting.html).\n",
    "\n",
    "Here are some examples where boradcasting will fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efe98645",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment to see examples:\n",
    "# a = torch.ones(4, 3, 2)\n",
    "\n",
    "# b = a * torch.rand(4, 3) # dimensions must match last to first\n",
    "\n",
    "# c = a * torch.rand(2, 3) # both 3rd and 2nd dimensions differ\n",
    "\n",
    "# d = a * torch.rand((0, )) # empty tensors cannot be broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0dcb75",
   "metadata": {},
   "source": [
    "## More Math with Tensors\n",
    "PyTorch provides over three hundred operations for tensor manipulation. A more detialed inventory is available [here](https://pytorch.org/docs/stable/torch.html#math-operations).\n",
    "\n",
    "Here are some sample operations often encountered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e64397a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Common functions:\n",
      "tensor([[0.4183, 0.6450, 0.1114, 0.7541],\n",
      "        [0.9275, 0.5390, 0.9243, 0.5521]])\n",
      "tensor([[1., -0., -0., -0.],\n",
      "        [1., 1., -0., -0.]])\n",
      "tensor([[ 0., -1., -1., -1.],\n",
      "        [ 0.,  0., -1., -1.]])\n",
      "tensor([[ 0.4183, -0.5000, -0.1114, -0.5000],\n",
      "        [ 0.5000,  0.5000, -0.5000, -0.5000]])\n",
      "\n",
      "Sine and Arcsine:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 0.7854])\n",
      "\n",
      "Broadcasted, element-wise equality comparison:\n",
      "tensor([[ True, False],\n",
      "        [False, False]])\n",
      "\n",
      "Reduction operations:\n",
      "tensor(4.)\n",
      "4.0\n",
      "tensor(2.5000)\n",
      "tensor(1.2910)\n",
      "tensor(24.)\n",
      "tensor([1, 2])\n",
      "\n",
      "Vectors & matrices:\n",
      "tensor([ 0.,  0., -1.])\n",
      "tensor([[0.6772, 0.5274],\n",
      "        [0.6325, 0.0910]])\n",
      "tensor([[2.0317, 1.5822],\n",
      "        [1.8975, 0.2729]])\n",
      "torch.return_types.svd(\n",
      "U=tensor([[-0.8142, -0.5805],\n",
      "        [-0.5805,  0.8142]]),\n",
      "S=tensor([3.1125, 0.7864]),\n",
      "V=tensor([[-0.8854,  0.4648],\n",
      "        [-0.4648, -0.8854]]))\n"
     ]
    }
   ],
   "source": [
    "# Common functions:\n",
    "a = torch.rand(2, 4) * 2 - 1\n",
    "print (\"\\nCommon functions:\")\n",
    "print (torch.abs(a))\n",
    "print (torch.ceil(a))\n",
    "print (torch.floor(a))\n",
    "print (torch.clamp(a, -0.5, 0.5))\n",
    "\n",
    "# Trigonometric functions and their inverse\n",
    "angles = torch.tensor([0, math.pi/4, math.pi/2, 3*math.pi/4])\n",
    "sines = torch.sin(angles)\n",
    "inverses = torch.asin(sines)\n",
    "print (\"\\nSine and Arcsine:\")\n",
    "print (angles)\n",
    "print (sines)\n",
    "print (inverses)\n",
    "\n",
    "# Comparisons:\n",
    "print (\"\\nBroadcasted, element-wise equality comparison:\")\n",
    "d = torch.tensor(([1., 2.], [3., 4.]))\n",
    "e = torch.ones(1, 2) # many comparison operations support broadcasting\n",
    "print (torch.eq(d, e)) # returns a tensor with bool type\n",
    "\n",
    "# Reductions:\n",
    "print (\"\\nReduction operations:\")\n",
    "print (torch.max(d))           # returns a single-element tensor\n",
    "print (torch.max(d).item())    # extracts the value from the returned tensor\n",
    "print (torch.mean(d))          # average\n",
    "print (torch.std(d))           # standard deviation\n",
    "print (torch.prod(d))          # product of all numbers\n",
    "print (torch.unique(torch.tensor([1,2,1,2,1,2,1,2,1,2]))) # filters for unique numbers\n",
    "\n",
    "# Vector and linear algebra operations:\n",
    "v1 = torch.tensor([1., 0., 0.])\n",
    "v2 = torch.tensor([0., 1., 0.])\n",
    "m1 = torch.rand(2,2)\n",
    "m2 = torch.eye(2)*3\n",
    "\n",
    "print(\"\\nVectors & matrices:\")\n",
    "print (torch.cross(v2, v1)) # negative of z unit vector (v1 x v2 == -v2 x v1)\n",
    "print (m1)\n",
    "m3 = torch.matmul(m1, m2)\n",
    "print (m3) # matmul 3 times m1\n",
    "print (torch.svd(m3)) # single value decomp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee23d9de",
   "metadata": {},
   "source": [
    "## Altering Tensors in Place:\n",
    "Most binary operations on tensors will return a third new tensor. When we say `c = a * b` (where `a` and `b` are tensors), the new tensor `c` will occupy a region of memory distinct from the other tensors.\n",
    "\n",
    "There are times, though, that we may wish to alter tensors in place - for example, if we do element-wise computation where we can discard intermediate values. For this, most of the math functions have a version with an appended underscore (\\_) that will alter the tensor in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b481af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "After operation:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "b:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "After operation:\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0, math.pi/4, math.pi/2, 3*math.pi/4])\n",
    "print ('a:')\n",
    "print (a)\n",
    "print (torch.sin(a)) # this operation creates a new tensor in memory\n",
    "print ('After operation:')\n",
    "print (a) # a is unchanged\n",
    "\n",
    "b = torch.tensor([0, math.pi/4, math.pi/2, 3*math.pi/4])\n",
    "print ('b:')\n",
    "print (b)\n",
    "print (torch.sin_(b)) # this operation modeifies the old tnsor in-place \n",
    "print ('After operation:')\n",
    "print (b) # b is changed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691bb8ab",
   "metadata": {},
   "source": [
    "For arithmatic operations, there are functions that behave similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17c4b771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]]) \n",
      " tensor([[0.2323, 0.7269],\n",
      "        [0.1187, 0.3951]])\n",
      "\n",
      "After adding:\n",
      "tensor([[1.2323, 1.7269],\n",
      "        [1.1187, 1.3951]])\n",
      "tensor([[1.2323, 1.7269],\n",
      "        [1.1187, 1.3951]]) \n",
      " tensor([[0.2323, 0.7269],\n",
      "        [0.1187, 0.3951]])\n",
      "\n",
      "After multiplying:\n",
      "tensor([[0.2862, 1.2552],\n",
      "        [0.1329, 0.5512]])\n",
      "tensor([[0.2862, 1.2552],\n",
      "        [0.1329, 0.5512]]) \n",
      " tensor([[0.2323, 0.7269],\n",
      "        [0.1187, 0.3951]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,2)\n",
    "b = torch.rand(2,2)\n",
    "\n",
    "print ('Before:')\n",
    "print (a, '\\n', b)\n",
    "print ('\\nAfter adding:')\n",
    "print (a.add_(b)) # in-place addition of b to a\n",
    "print (a, '\\n', b)\n",
    "print ('\\nAfter multiplying:')\n",
    "print (a.mul_(b)) # in-place multiplication of b to a\n",
    "print (a, '\\n', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb83eb",
   "metadata": {},
   "source": [
    "Note that these in-place arithmatic functions are methods on the `torch.Tensor` object, not attached to the `torch` module like many other functions (e.g. `torch.sin()`). As we can see from `a.add_(b)`, _the calling tensor_ is the one that gets changed in place.\n",
    "\n",
    "There is another option for placing the results of a computation in an existing, allocated tensor. Many of the methods and functions we've seen so far - including creation methods - have an `out` argument that allows specification of an output tensor. If the `out` tensor is the correct shape and `dtype`, this can happen without new memory allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fe95551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0.7960, 0.9876],\n",
      "        [0.6180, 0.8033]])\n",
      "tensor([[0.9874, 0.7316],\n",
      "        [0.2814, 0.0651]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "c = torch.zeros(2, 2)\n",
    "old_id = id(c)\n",
    "\n",
    "print(c)\n",
    "d = torch.matmul(a, b, out=c)\n",
    "print(c)                # contents of c have changed\n",
    "\n",
    "assert c is d           # test c & d are same object, not just containing equal values\n",
    "assert id(c), old_id    # make sure that our new c is the same object as the old one\n",
    "\n",
    "torch.rand(2, 2, out=c) # works for creation too!\n",
    "print(c)                # c has changed again\n",
    "assert id(c), old_id    # still the same object!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e824f33",
   "metadata": {},
   "source": [
    "## Copying Tensors:\n",
    "As with any object in Python, assigning a tensor to a variable makes the variable a _label_ of the tensor, and does not copy it. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11892e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1., 561.],\n",
      "        [  1.,   1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,2)\n",
    "b = a\n",
    "\n",
    "a[0][1] = 561 # changes made to a are also seen when b is called. \n",
    "print (b) # b is only a label to the tensor object. Not an independant tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b66965b",
   "metadata": {},
   "source": [
    "To create a separate copy, the `clone()` method can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35befd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([[  1.,   1.],\n",
      "        [  1., 200.]])\n",
      "b:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,2)\n",
    "b = a.clone()\n",
    "\n",
    "assert b is not a\n",
    "\n",
    "a[1][1] = 200\n",
    "print ('a:')\n",
    "print (a)\n",
    "print ('b:')\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57665e11",
   "metadata": {},
   "source": [
    "__An important note on the__ `clone()` __method.__ If the source tensor has autograd enabled, then so will the clone. In many cases, this will be beneficial. For example, if the model has multiple computation paths in the `forward()` method, and _both_ the source and clone contribute to the model output, then enabling autograd in both is necessary for learning. If the source already has autograd enabled, then enabling autgrad for the clone is trivial.\n",
    "\n",
    "On the other hand, if we're doing a computation where _neither_ the original tensor nor it clone need to track their gradients, then as long as the source tensor has autograd turned off, there are no problems.\n",
    "\n",
    "Issues arrise when we compute using the model's `forward()` function, where gradients are turned on by default, but want to pull out values mid-stream to generate metrics. In this case, we _don't_ want to turn off autograd for the source, but need autograd to be off for the cloned matrices. For this, we can use the `.detach()` method on the source tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "807932fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0065, 0.5035],\n",
      "        [0.3082, 0.3742]], requires_grad=True)\n",
      "tensor([[0.0065, 0.5035],\n",
      "        [0.3082, 0.3742]], grad_fn=<CloneBackward0>)\n",
      "tensor([[0.0065, 0.5035],\n",
      "        [0.3082, 0.3742]])\n",
      "tensor([[0.0065, 0.5035],\n",
      "        [0.3082, 0.3742]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2, requires_grad = True) # turn on autograd\n",
    "print (a)\n",
    "\n",
    "b = a.clone()\n",
    "print (b)\n",
    "\n",
    "c = a.detach().clone()\n",
    "print (c)\n",
    "\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0230a0c6",
   "metadata": {},
   "source": [
    "What's happening here?\n",
    "- We create `a`, with `requires_grad=True` turned on. \n",
    "- When we print `a`, it informs us that the property `requires_grad=True` - this means that autograd and computation history tracking are turned on\n",
    "- We clone `a` and label it `b`. When we print `b`, we can see that it's tracking its computation history - it has inherited `a`'s autograd settings, and added to the computation history.\n",
    "- We clone `a` to `c`, but we call `.detach()` first.\n",
    "- Printing `c`, we see no computation history, and no `requires_grad=True`.\n",
    "\n",
    "The `detach()` method _detaches the tensor from its computation history_. It say \"do what comes next as if autograd was off\". It does this _without_ changing the original properties, so when we print `a` again after cloning `c`, we see that `requires_grad=True` remains unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64c26d2",
   "metadata": {},
   "source": [
    "## Moving to GPU:\n",
    "One of the major advantages of PyTorch is its robust acceleration on CUDA-compatible Nvidia GPUs. (\"CUDA\" stands for _Compute Unified Device Architecture_, which is Nvidia's platform for parallel computing.) So far, everything so far has been done on the CPU. For heavier computations, moving to a GPU is more efficient.\n",
    "\n",
    "First, we need to check if a CUDA-compatible GPU is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bd7d8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device available.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print ('CUDA device available.')\n",
    "else:\n",
    "    print ('No CUDA device.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113149d7",
   "metadata": {},
   "source": [
    "Once we've determined that one or more GPUs is available, we need to move our data to where the GPU can see. CPU computations access data on the RAM. GPUs have a dedicated memory attached. Data must be moved from the RAM to GPU memory before it can be processed by the GPU.\n",
    "\n",
    "There are multiple ways to get data onto the target device. We can do it at creation time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37c03c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1272, 0.8167],\n",
      "        [0.5440, 0.6601]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_rand = torch.rand(2,2, device='cuda')\n",
    "    print (gpu_rand)\n",
    "else:\n",
    "    print ('No CUDA device.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e2e25",
   "metadata": {},
   "source": [
    "By default, new tensors are created on the CPU, so we have to specify when we want to create our tensor on the GPU with the optional `device` argument. You can see when we print the new tensor, PyTorch informs us which device it's on (if not on CPU).\n",
    "\n",
    "We can query the number GPUs available with `torch.cuda.device_count()`. If we have more than one GPU, we can specify them by index: `device = 'cuda:0'`, `device = 'cuda:1'`, etc.\n",
    "\n",
    "As a coding practice, specifying our devices everywhere with string constants is pretty fragile. In an ideal world, code would perform robustly whether on a CPU or a GPU hardware. We can do this by creating a device handle that can be passed to tensors instead of a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b3236cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is \"cuda\".\n",
      "tensor([[0.6208, 0.0276],\n",
      "        [0.3255, 0.1114]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print (f'Device is \"{device}\".')\n",
    "\n",
    "x = torch.rand(2,2, device=device)\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beb028f",
   "metadata": {},
   "source": [
    "If we have an existing tensor already on one device, we can move it to another using the `to()` method. The following code creates a tensor at the default CPU device, then moves it to the device handle we defined in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2770da80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4297, 0.9729],\n",
      "        [0.9739, 0.4533]])\n",
      "y device is : cuda:0\n",
      "y_cou device is: cpu\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(2, 2)\n",
    "print (y)\n",
    "y = y.to(device)\n",
    "print (f'y device is : {y.device}')\n",
    "\n",
    "y_cpu = y.clone()\n",
    "y_cpu = y_cpu.to(\"cpu\")\n",
    "print (f'y_cou device is: {y_cpu.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b443d313",
   "metadata": {},
   "source": [
    "It is important to note that computations can only be done if the device has access to both tensors in memory. I.e., both tensors have to be on the same CUDA device, or CPU to interact with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0aa67d",
   "metadata": {},
   "source": [
    "## Manipulating Tensor Shapes:\n",
    "Sometimes, we need to change the shapes of our tensors. Below, we'll look at a few common cases, and how to handle them.\n",
    "\n",
    "### Changing the Number of Dimensions:\n",
    "One case where we might need to change the number of dimensions is passing a single instance of input to the model; one picture from a batch, or one example from a set. PyTorch models generally expect _batches_ of inputs.\n",
    "\n",
    "For example, imagine a model that works on 226x226 pixel coloured images. When we load and transform it, we get a tensor of shape `(3, 226, 226)` for each picture fed into the model. The model then expects the batch to have the shape of `(N, 3, 226, 226)`, where `N` is the number of items in the batch. To make a batch of one, we use the `unsqueeze` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa01c091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 226, 226])\n",
      "torch.Size([1, 3, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 226, 226)\n",
    "b = a.unsqueeze(0)\n",
    "\n",
    "print (a.shape)\n",
    "print (b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab03f4de",
   "metadata": {},
   "source": [
    "The `unsqueeze()` method adds a dimension of extent 1. Specifying `unsqueeze(0)` adds is as a new dimension zero, pushing the rest back by one. This gives us a batch with one item of 3x226x226.\n",
    "\n",
    "So if that is _unsqueezing_, what is _squeezing_? `squeeze` takes advantage of the fact that any dimension of extent 1 _does not_ change the number of elements in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22f44f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[[0.5017]]]]]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.rand(1,1,1,1,1,1)\n",
    "print (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db907c",
   "metadata": {},
   "source": [
    "Continuing the example above, let's say the model's output is a 20-element vector for each input. We would then expect the output to have a shape `(N, 20)`, where `N` is the number of instances in the input batch. That means that for our single-input batch, we'll get an output shape of `(1, 20)`.\n",
    "\n",
    "What if we want to do some _non-batched_ computation with a single output - operations that expect a 20-element vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aed79216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of a is: torch.Size([1, 20])\n",
      "a:\n",
      " tensor([[0.8647, 0.8954, 0.4120, 0.2145, 0.1577, 0.3815, 0.1463, 0.5738, 0.1817,\n",
      "         0.0133, 0.2362, 0.0667, 0.4781, 0.5967, 0.2972, 0.1862, 0.7488, 0.0809,\n",
      "         0.3406, 0.7557]])\n",
      "shape of b is: torch.Size([20])\n",
      "b:\n",
      " tensor([0.8647, 0.8954, 0.4120, 0.2145, 0.1577, 0.3815, 0.1463, 0.5738, 0.1817,\n",
      "        0.0133, 0.2362, 0.0667, 0.4781, 0.5967, 0.2972, 0.1862, 0.7488, 0.0809,\n",
      "        0.3406, 0.7557])\n",
      "shape of c is: torch.Size([2, 2])\n",
      "shape of d is: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1, 20)\n",
    "print (f'shape of a is: {a.shape}')\n",
    "print ('a:\\n', a)\n",
    "\n",
    "b = a.squeeze(0)\n",
    "print (f'shape of b is: {b.shape}')\n",
    "print ('b:\\n', b)\n",
    "\n",
    "c = torch.rand(2, 2)\n",
    "print (f'shape of c is: {c.shape}')\n",
    "\n",
    "d = c.squeeze(0)\n",
    "print (f'shape of d is: {d.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a42b85",
   "metadata": {},
   "source": [
    "We can see from the shape that the 2-dimensional tensor `a` is now a one dimensional tensor `b`. The square brackets `[]` denoted the extra dimensions, with `a` having an extra set, and `b` without.\n",
    "\n",
    "`squeeze()` is only usable with dimensions of extent 1. Calling the `squeeze()` method on a dimension of size 2 in `c` returns a tensor with the original shape. \n",
    "\n",
    "Another way to use the `unsqueeze()` method is to ease broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d0d46b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1740, 0.1740],\n",
      "         [0.6526, 0.6526],\n",
      "         [0.0788, 0.0788]],\n",
      "\n",
      "        [[0.1740, 0.1740],\n",
      "         [0.6526, 0.6526],\n",
      "         [0.0788, 0.0788]],\n",
      "\n",
      "        [[0.1740, 0.1740],\n",
      "         [0.6526, 0.6526],\n",
      "         [0.0788, 0.0788]],\n",
      "\n",
      "        [[0.1740, 0.1740],\n",
      "         [0.6526, 0.6526],\n",
      "         [0.0788, 0.0788]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4,3,2)\n",
    "\n",
    "c = a * torch.rand(3, 1)\n",
    "print (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7237d590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n",
      "torch.Size([3])\n",
      "torch.Size([3, 1])\n",
      "tensor([[[0.1818, 0.1818],\n",
      "         [0.0603, 0.0603],\n",
      "         [0.5349, 0.5349]],\n",
      "\n",
      "        [[0.1818, 0.1818],\n",
      "         [0.0603, 0.0603],\n",
      "         [0.5349, 0.5349]],\n",
      "\n",
      "        [[0.1818, 0.1818],\n",
      "         [0.0603, 0.0603],\n",
      "         [0.5349, 0.5349]],\n",
      "\n",
      "        [[0.1818, 0.1818],\n",
      "         [0.0603, 0.0603],\n",
      "         [0.5349, 0.5349]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4, 3, 2)\n",
    "b = torch.rand(3)\n",
    "print (a.shape)\n",
    "print (b.shape)\n",
    "\n",
    "b = b.unsqueeze(1)\n",
    "print (b.shape)\n",
    "\n",
    "c = a*b\n",
    "print (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcb5f65",
   "metadata": {},
   "source": [
    "The `squeeze()` and `unsqueeze()` methods have in-place versions `squeeze_()` and `unsqueeze_()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80b97c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, 3)\n",
    "x.unsqueeze_(0)\n",
    "print (x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b247cdd",
   "metadata": {},
   "source": [
    "Sometimes we'll want to more radically change the shapes of tensors, while still preserving the number of elements and their contents. One case where this happens is at the interface between a convolutional layer and the linear layer of a model; this is common in image classification systems. A convolutional kernel yields an output tensor of shape _features x width x height_, but the following linear layer expects a 1-dimensional input. The `reshape()` method is the solution, provided that the dimensions we request yield the same number of elements as the input tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d236cd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 50, 50])\n",
      "torch.Size([7500])\n",
      "torch.Size([7500])\n"
     ]
    }
   ],
   "source": [
    "output3d = torch.rand(3, 50, 50)\n",
    "print (output3d.shape)\n",
    "\n",
    "input1d = output3d.reshape(3*50*50)\n",
    "print (input1d.shape)\n",
    "\n",
    "print (torch.reshape(output3d, (3*50*50,)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad3e59c",
   "metadata": {},
   "source": [
    "Note: the argument `(3*50*50,)` expects a __tuple__ when specifying a tensor shape, but when the shape is the first argument of a method, we can cheat with a series of integers. Here we add the parantheses and comma to convince the method that this is a one-element tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfa5afe",
   "metadata": {},
   "source": [
    "When it can, `reshape()` will return a _view_ on the tensor being changed - that is, a separate tensor object looking at the same underlying region of the memory. _This is important_: That means any change made to the source tensor will be reflected in the view on that tensor, unless we `clone()` it.\n",
    "\n",
    "There _are_ conditions beyond the scope of this introduction where `reshape()` has to return a tensor carrying a copy of the data. More information can be found [here](https://pytorch.org/docs/stable/torch.html#torch.reshape)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68472255",
   "metadata": {},
   "source": [
    "## NumPy Bridge:\n",
    "In the section above on boradcasting, it was mentioned that PyTorch's boradcast semantics are compatible with NumPy's - but the kinship between PyTorch and NumPy goes even deeper than that.\n",
    "\n",
    "If we have existing ML or scientific code with data stored in NumPy ndarrays, we may wish to express that same data as PyTorch tensors, whether to take advantage of PyTorch's GPU acceleration, of its efficient abstractions for building ML models. It's easy to switch between ndarrays and PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f834f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numpy_array = np.ones((2,3))\n",
    "print (numpy_array)\n",
    "\n",
    "pytorch_tensor = torch.from_numpy(numpy_array)\n",
    "print (pytorch_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bdc4e6",
   "metadata": {},
   "source": [
    "PyTorch creates a tensor of the same shape and containing the same data as the NumPy array, going as far as to keep NumPy's default 64-bit float data type.\n",
    "\n",
    "The conversion can just as easily go the other way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a5dfb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8216, 0.8794, 0.8226],\n",
      "        [0.7592, 0.7540, 0.8187]])\n",
      "[[0.82161367 0.87936646 0.82257426]\n",
      " [0.7592339  0.75399923 0.818666  ]]\n"
     ]
    }
   ],
   "source": [
    "pytorch_rand = torch.rand(2, 3)\n",
    "print (pytorch_rand)\n",
    "\n",
    "numpy_rand = pytorch_rand.numpy()\n",
    "print (numpy_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38a903e",
   "metadata": {},
   "source": [
    "It is important to know that these converted objevts are using _the same underlying memory_ as their source pbjects, meaning that changes to one are reflected in the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4535ebe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1., 23.,  1.]], dtype=torch.float64)\n",
      "[[ 0.82161367  0.87936646  0.82257426]\n",
      " [ 0.7592339  17.          0.818666  ]]\n"
     ]
    }
   ],
   "source": [
    "numpy_array[1,1] = 23\n",
    "print (pytorch_tensor)\n",
    "\n",
    "pytorch_rand[1,1] = 17\n",
    "print (numpy_rand)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
