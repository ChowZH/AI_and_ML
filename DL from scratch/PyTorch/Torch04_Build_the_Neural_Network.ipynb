{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a3c5ea",
   "metadata": {},
   "source": [
    "# Build the Neural Network\n",
    "Neural networks comproses of layers/modules that perform operations on data. The [torch.nn](https://pytorch.org/docs/stable/nn.html) namespace provides all the building blocks for neural network design. Every module in Pytorch subclasses the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) is a neural network module that consists of other layers. This nested structure allows for building and managing complex achitectures.\n",
    "\n",
    "In the following sections contribute to a sample neural network to classify images from the FashionMNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f3b0335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The imports\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1731420",
   "metadata": {},
   "source": [
    "## Get Device for Training\n",
    "We want to be able to train our model on a hardware accelerator like a GPU, if it is available. To do this, we use [torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html) to check if one is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e5b8c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d084ce4d",
   "metadata": {},
   "source": [
    "## Define the class\n",
    "We define a neural network by subclassing `nn.Module`, and initializing the neural network layers in `__init__`. Every `nn.Module` subclass implements the operations on input data in the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a1969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a00899",
   "metadata": {},
   "source": [
    "We create an instance of `NeuralNetwork`, and move it to the `device`, and print its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c910d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a0cb3f",
   "metadata": {},
   "source": [
    "To use this model, we pass the input data. This executes the model's `forward`, along with some [background operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866)(including automated back-propogation). Note: Do not call `model.forward()` directly unless skipping background operations is intended.\n",
    "\n",
    "Calling the model on the input returns a 10-dimensional tensor with raw predicted values for each class. We get the prediction probabilities by passing it through an instance of the `nn.Softmax` module (usually included in the NeuralNetwork depending on the setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4e35749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([5], device='cuda:0').\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_prob = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_prob.argmax(1)\n",
    "print (f\"Predicted class: {y_pred}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddedd9d",
   "metadata": {},
   "source": [
    "## Model Layers\n",
    "Let's break down the layers in the FashionMNIST model. To illustrate it, we will take a sample minibatch of 3 images of size 28x28 each and see what happens to it as we pass it through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b757eca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print (input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a520a",
   "metadata": {},
   "source": [
    "### nn.Flatten\n",
    "We initialize the `nn.Flatten` layer and convert each 2D 28x28 image to a contiguous array of 784 pixel values (the minibatch dimension (at dim=0) is maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "695daebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print (flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf85354",
   "metadata": {},
   "source": [
    "### nn.Linear\n",
    "The [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) is a module that applies a linear transformation on the input using its stored weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35bbb873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features = 20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print (hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f628ff",
   "metadata": {},
   "source": [
    "### nn.ReLU\n",
    "Non-linear activations are what create the complex mappings between model's inputs and outputs. They are applied after linear transformations to introduce *non-linearity* (stacking only linear opperations result in linear transformations, thus being ultimately useless. Non-linearity allows the model to represent complex functions.), helping the neural network learn a wide variety of phenomena.\n",
    "\n",
    "In this model we use [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) between our linear layers, but there are other non-linear functions like `nn.ELU`, `nn.LeakyReLU`, `nn.Tanh`, `nn.GLU`, etc. More information about non-linear activations can be found in the [torch.nn](https://pytorch.org/docs/stable/nn.html) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9246fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-5.0557e-01, -1.7142e-01,  6.6578e-01,  2.5479e-01,  3.3269e-01,\n",
      "         -1.5199e-01, -3.9121e-01, -6.0498e-02,  1.1474e-01,  2.1987e-01,\n",
      "         -4.8143e-01,  7.0115e-01,  3.7391e-01,  7.6089e-02,  5.5664e-01,\n",
      "         -8.2708e-02,  4.5848e-01, -2.1165e-01, -7.7354e-02, -4.5861e-01],\n",
      "        [-3.7117e-01, -3.2021e-04,  4.4860e-01,  6.6740e-01,  9.8738e-01,\n",
      "          1.3331e-01, -7.4467e-01, -1.4885e-01,  5.4119e-02, -9.8745e-02,\n",
      "         -7.4164e-01,  3.5549e-01,  3.2751e-01, -1.2333e-01,  7.7946e-01,\n",
      "          2.1716e-01,  2.8492e-01, -5.5487e-02,  4.8754e-02, -3.9980e-01],\n",
      "        [-4.9006e-01,  6.0524e-02,  3.1122e-01,  7.8763e-01,  3.9474e-01,\n",
      "          6.7923e-02, -5.8953e-01,  7.0271e-02,  5.9303e-02,  1.1663e-01,\n",
      "         -3.8154e-01,  5.9758e-01, -1.5838e-01, -1.6142e-01,  7.2429e-01,\n",
      "          5.3106e-02,  5.0966e-01, -2.9236e-01,  1.9882e-01, -3.1382e-01]],\n",
      "       grad_fn=<AddmmBackward0>).\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.0000, 0.6658, 0.2548, 0.3327, 0.0000, 0.0000, 0.0000, 0.1147,\n",
      "         0.2199, 0.0000, 0.7012, 0.3739, 0.0761, 0.5566, 0.0000, 0.4585, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.4486, 0.6674, 0.9874, 0.1333, 0.0000, 0.0000, 0.0541,\n",
      "         0.0000, 0.0000, 0.3555, 0.3275, 0.0000, 0.7795, 0.2172, 0.2849, 0.0000,\n",
      "         0.0488, 0.0000],\n",
      "        [0.0000, 0.0605, 0.3112, 0.7876, 0.3947, 0.0679, 0.0000, 0.0703, 0.0593,\n",
      "         0.1166, 0.0000, 0.5976, 0.0000, 0.0000, 0.7243, 0.0531, 0.5097, 0.0000,\n",
      "         0.1988, 0.0000]], grad_fn=<ReluBackward0>).\n"
     ]
    }
   ],
   "source": [
    "print (f\"Before ReLU: {hidden1}.\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print (f\"After ReLU: {hidden1}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f99b9c6",
   "metadata": {},
   "source": [
    "### nn.Sequential\n",
    "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network like `seq_modules`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb502983",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20,10),\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bd9aa2",
   "metadata": {},
   "source": [
    "### nn.Softmax\n",
    "The last layer of the neural network returns *logits* - raw values in \\[-infty, infty\\] - which are passed to the [nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) module. The logits are scaled to values \\[0,1\\] (which is the [softargmax](https://en.wikipedia.org/wiki/Softmax_function)) representing the model's prediction for each class. `dim` parameter indicates the dimension along which the values must sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beca544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_softargmax = nn.Softmax\n",
    "softargmax = nn_softargmax(dim=1)\n",
    "pred_prob = softargmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e814673",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "Many layers inside a neural network are *parameterized*, i.e. have associated weights and biases that are optimized during training. Subclassing `nn.Module` automatically tracks all fields defined inside the model object, and makes all the parameters accissible using the model's `parameters()` or `named_parameters()` methods.\n",
    "\n",
    "In this example, we iterate over each parameter, and print its size and a preview of its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d12f367a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values: tensor([[ 0.0198,  0.0242, -0.0213,  ...,  0.0192,  0.0120, -0.0115],\n",
      "        [-0.0097, -0.0106, -0.0096,  ..., -0.0047,  0.0187,  0.0199]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values: tensor([0.0021, 0.0158], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values: tensor([[-0.0375,  0.0182,  0.0351,  ...,  0.0093, -0.0031, -0.0367],\n",
      "        [-0.0256,  0.0203,  0.0315,  ..., -0.0142, -0.0001, -0.0412]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values: tensor([-0.0040, -0.0238], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values: tensor([[ 0.0114,  0.0410,  0.0149,  ..., -0.0327,  0.0280, -0.0117],\n",
      "        [-0.0150,  0.0414, -0.0040,  ...,  0.0110,  0.0196,  0.0070]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values: tensor([-0.0292, -0.0228], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print (f\"Layer: {name} | Size: {param.size()} | Values: {param[:2]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
