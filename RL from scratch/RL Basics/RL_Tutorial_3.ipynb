{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "741e570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial 3: Deep Q-Learning\n",
    "\n",
    "# Building a DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d84d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
    "from keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12aabc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLAY_MEMORY_SIZE = 50_000\n",
    "MIN_REPLAY_MEMORY_SIZE = 1_000\n",
    "MODEL_NAME = \"256x2\"\n",
    "MINIBATCH_SIZE = 64\n",
    "DISCOUNT = 0.99\n",
    "UPDATE_TARGET_EVERY = 5\n",
    "EPISODES = 20_000\n",
    "MAX_EPISODE_STEP = 200\n",
    "\n",
    "epsilon = 1\n",
    "EPSILON_DECAY = 0.9999\n",
    "MIN_EPSILON = 0.001\n",
    "\n",
    "AGGREGATE_STATS_EVERY = 50\n",
    "SHOW_PREVIEW = False\n",
    "\n",
    "                \n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        # Main model\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "        # Target model\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.replay_memory = deque(maxlen = REPLAY_MEMORY_SIZE)\n",
    "    \n",
    "        self.target_update_counter = 0\n",
    "    \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(256, (3,3), input_shape=env.OBSERVATION_SPACE_VALUES))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(2, 2))\n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "        model.add(Conv2D(256, (3,3)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(2, 2))\n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64))\n",
    "        \n",
    "        model.add(Dense(env.ACTION_SPACE_SIZE, activation=\"linear\"))\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "        \n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]\n",
    "    \n",
    "    def train(self, terminal_state, step):\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "        \n",
    "        current_states = np.array([transition[0] for transition in minibatch])/255\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "        \n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])/25\n",
    "        future_qs_list  =self.target_model.predict(new_current_states)\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT*max_future_q\n",
    "                \n",
    "            else:\n",
    "                new_q = reward\n",
    "                \n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "            \n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "            \n",
    "        self.model.fit(np.array(X)/255, np.array(y), batch_size = MINIBATCH_SIZE,\n",
    "                       verbose = 0, shuffle = False if terminal_state else None)\n",
    "        \n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "            \n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d72ea8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlobEnv:\n",
    "    SIZE = 10\n",
    "    RETURN_IMAGES = True\n",
    "    MOVE_PENALTY = 1\n",
    "    ENEMY_PENALTY = 3000\n",
    "    FOOD_REWARD = 25\n",
    "    OBSERVATION_SPACE_VALUES = (SIZE, SIZE, 3)\n",
    "    ACTION_SPACE_SIZE = 9 # 8 cardinal directions + no move\n",
    "    PLAYER_N = 1\n",
    "    FOOD_N = 2\n",
    "    ENEMY_N = 3\n",
    "    \n",
    "    d = {1:(255, 175, 0),\n",
    "         2:(0, 255, 0),\n",
    "         3:(0, 0, 255)}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.player = Blob(self.SIZE)\n",
    "        self.food = Blob(self.SIZE)\n",
    "        while self.food == self.player:\n",
    "            self.food = Blob(self.SIZE) # If food spawns on player, respawn food\n",
    "        self.enemy = Blob(self.SIZE)\n",
    "        while self.enemy == self.player or self.enemy == self.food:\n",
    "            self.enemy = Blob(self.SIZE) # If enemy spawns on food or player, respawn enemy\n",
    "        \n",
    "        self.episode_step = 0\n",
    "        \n",
    "        if self.RETURN_IMAGES:\n",
    "            observation = np.array(self.get_image())\n",
    "        else:\n",
    "            observation = (self.player-self.food) + (self.player-self.enemy)\n",
    "        return observation\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.episode_step += 1\n",
    "        self.player.action (action)\n",
    "        \n",
    "        ##\n",
    "        # Food or enemy move also goes in here\n",
    "        # Each step of the environment\n",
    "        ##\n",
    "        \n",
    "        if self.RETURN_IMAGES:\n",
    "            new_observation = np.array(self.get_image())\n",
    "        else:\n",
    "            new_observation = (self.player-self.food) + (self.player-self.enemy)\n",
    "            \n",
    "        if self.player == self.enemy:\n",
    "            reward = -self.ENEMY_PENALTY\n",
    "        elif self.player == self.food:\n",
    "            reward = +self.FOOD_REWARD\n",
    "        else:\n",
    "            reward = -self.MOVE_PENALTY\n",
    "            \n",
    "        done = False\n",
    "        if reward == self.FOOD_REWARD or reward == -self.ENEMY_PENALTY or self.episode_step >= MAX_EPISODE_STEP:\n",
    "            done = True\n",
    "        \n",
    "        return new_observation, reward, done\n",
    "    \n",
    "    def render(self):\n",
    "        img = self.get_image()\n",
    "        img = img.resize((300, 300))\n",
    "        cv2.imshow(\"image\", np.array(img))\n",
    "        cv2.waitKey(1)\n",
    "        \n",
    "    # For CNN\n",
    "    def get_image(self):\n",
    "        env = np.zeros((self.SIZE, self.SIZE, 3), dtype=np.uint8)\n",
    "        env[self.food.x][self.food.y] = self.d[self.FOOD_N]\n",
    "        env[self.enemy.x][self.enemy.y] = self.d[self.ENEMY_N]\n",
    "        env[self.player.x][self.player.y] = self.d[self.PLAYER_N]\n",
    "        img = Image.fromarray(env,'RGB')\n",
    "        return img\n",
    "        \n",
    "        \n",
    "class Blob:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.x = np.random.randint(0, size)\n",
    "        self.y = np.random.randint(0, size)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.x, {self.y}}\"\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return (self.x - other.x, self.y - other.y)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "    \n",
    "    def action(self, choice):\n",
    "        if choice == 0: # Move up\n",
    "            self.move(x = 1, y = 0)\n",
    "        elif choice == 1: # Move down\n",
    "            self.move(x = -1, y = 0)\n",
    "        elif choice == 2: # Move right\n",
    "            self.move(x = 0, y = 1)\n",
    "        elif choice == 3: # Move left\n",
    "            self.move(x = 0, y = -1)\n",
    "        elif choice == 4: # Move up-right\n",
    "            self.move(x = 1, y = 1)\n",
    "        elif choice == 5: # Move up-left\n",
    "            self.move(x = 1, y = -1)\n",
    "        elif choice == 6: # Move down-right\n",
    "            self.move(x = -1, y = 1)\n",
    "        elif choice == 7: # Move down-left\n",
    "            self.move(x = -1, y = -1)\n",
    "        elif choice == 8: # Don't move\n",
    "            self.move(x = 0, y = 0)\n",
    "    \n",
    "    def move(self, x=False, y=False):\n",
    "        if not x:\n",
    "            self.x += np.random.randint(-1,2)\n",
    "        else:\n",
    "            self.x += x\n",
    "            \n",
    "        if not y:\n",
    "            self.y += np.random.randint(-1,2)\n",
    "        else:\n",
    "            self.y += y\n",
    "    \n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "        elif self.x > self.size-1:\n",
    "            self.x = self.size-1\n",
    "            \n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "        elif self.y > self.size-1:\n",
    "            self.y = self.size-1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de827d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Toffee\\.conda\\envs\\Reinforcement_Learning\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/20000 [00:00<?, ?episode/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/256x2__-200.00max_-200.00avg_-200.00min__1657619033.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|#6        | 3206/20000 [9:29:31<18:52:08,  4.04s/episode] "
     ]
    }
   ],
   "source": [
    "# For stats\n",
    "ep_rewards = [-200]\n",
    "\n",
    "# For model save\n",
    "MIN_REWARD = -200\n",
    "\n",
    "# For more repetitive results (seeds fix random values for hyperparameter tuning)\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "env = BlobEnv()\n",
    "agent = DQNAgent()\n",
    "\n",
    "for episode in tqdm(range(1, EPISODES+1), ascii = True, unit = \"episode\"):\n",
    "    \n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "    current_state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(agent.get_qs(current_state))\n",
    "        else:\n",
    "            action = np.random.randint(0, env.ACTION_SPACE_SIZE)\n",
    "            \n",
    "        new_state, reward, done = env.step(action)\n",
    "        \n",
    "        episode_reward += reward \n",
    "        \n",
    "        if SHOW_PREVIEW and not episode % AGGREGATE_STATS_EVERY:\n",
    "            env.render()\n",
    "            \n",
    "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "        agent.train(done, step)\n",
    "        \n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "        \n",
    "    ep_rewards.append(episode_reward)\n",
    "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        \n",
    "        if min_reward >= MIN_REWARD:\n",
    "            agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
    "            \n",
    "\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce9ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
